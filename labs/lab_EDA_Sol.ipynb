{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98e03cfa",
   "metadata": {},
   "source": [
    "# Lab 1: EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d051dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pingouin as pg\n",
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56ee7c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga el dataset de California Housing\n",
    "desc = fetch_california_housing()\n",
    "X,Y = fetch_california_housing(return_X_y = True)\n",
    "data = pd.DataFrame(X, columns =desc['feature_names'] )\n",
    "data[desc['target_names'][0]] = Y "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20a86bb",
   "metadata": {},
   "source": [
    "**Para el conjunto de datos dado, realice la implementacion de la siguiente estrategia de eliminacion de informacion redundante:**\n",
    "\n",
    "1.\tEntre las variables explicativas (todas las variables excepto la variable objetivo), identifique los pares con una correlación de Pearson superior al 80%. Para cada par, elimine la variable que presente una menor correlación parcial con la variable objetivo.\n",
    "2.\tAplique una estrategia adicional de reducción de redundancia (al conjunto de datos original) evaluando el índice de correlación múltiple entre las variables explicativas, y elimine aquellas cuya variabilidad explicada sea superior al 80% en funcion de las demas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8344d5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ejemplo correlacion parcial:\n",
    "# data.pcorr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35956852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlación máxima entre variables explicativas: 0.9246644339150362\n",
      "Pares de variables con correlación > 0.8: [('AveBedrms', 'AveRooms'), ('Longitude', 'Latitude')]\n",
      "Correlación máxima entre variables explicativas tras eliminación: 0.9246644339150362\n",
      "{'AveRooms'}\n",
      "Pares de variables con correlación > 0.8: [('Longitude', 'Latitude')]\n",
      "Correlación máxima entre variables explicativas tras eliminación: 0.29624423977353637\n",
      "{'Longitude'}\n"
     ]
    }
   ],
   "source": [
    "### TO DO 1: Eliminacion de reduncancia basado en par de variables\n",
    "\n",
    "### conjunto de datos dividido en variables explicativas y variable objetivo\n",
    "data_reg = data.copy()\n",
    "X_reg = data.drop(columns = ['MedHouseVal'])\n",
    "Y_reg = data['MedHouseVal']\n",
    "\n",
    "### Encontrar pares de variables con alta correlacion\n",
    "X_reg_reduced = X_reg.copy()\n",
    "corr_matrix = X_reg.corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "max_corr = upper_tri.max().max()\n",
    "print(\"Correlación máxima entre variables explicativas:\", max_corr)\n",
    "while max_corr > 0.8:\n",
    "    high_corr_pairs = [(col1, col2) for col1 in upper_tri.columns for col2 in upper_tri.index if upper_tri.loc[col2, col1] > 0.8]\n",
    "    print(\"Pares de variables con correlación > 0.8:\", high_corr_pairs)\n",
    "    ### Eliminar variables con menor correlacion parcial con la variable objetivo\n",
    "    to_drop = set()\n",
    "    for var1, var2 in [high_corr_pairs[0]]:\n",
    "        ### correlacion parcial con la variable objetivo\n",
    "        partial_corr = data_reg.pcorr()['MedHouseVal'].to_dict()\n",
    "        corr_var1 = partial_corr[var1]\n",
    "        corr_var2 = partial_corr[var2]\n",
    "        if abs(corr_var1) < abs(corr_var2):\n",
    "            to_drop.add(var1)\n",
    "        else:\n",
    "            to_drop.add(var2)\n",
    "    X_reg_reduced = X_reg_reduced.drop(columns = list(to_drop))\n",
    "    data_reg = data_reg.drop(columns = list(to_drop))\n",
    "    \n",
    "    corr_matrix = X_reg_reduced.corr().abs()\n",
    "    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    max_corr = upper_tri.max().max()\n",
    "    print(\"Correlación máxima entre variables explicativas tras eliminación:\", max_corr)\n",
    "    print(to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f40534b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Índice de correlación múltiple para cada variable explicativa:\n",
      "[0.60020701 0.1943632  0.88013596 0.85704064 0.12136195 0.00825572\n",
      " 0.89244564 0.88842104]\n",
      "Eliminando variable Latitude con índice de correlación múltiple 0.8924456441475132\n",
      "Nuevo índice de correlación múltiple máximo tras eliminación: 0.866904134133273\n",
      "Eliminando variable AveRooms con índice de correlación múltiple 0.866904134133273\n",
      "Nuevo índice de correlación múltiple máximo tras eliminación: 0.12013044283971674\n"
     ]
    }
   ],
   "source": [
    "### TO DO 2: Eliminacion de reduncancia basado en multiples variables\n",
    "\n",
    "### Calculo del indice de correlacion multiple\n",
    "sigma = X_reg.cov()\n",
    "sigma_inv = np.linalg.inv(sigma)\n",
    "\n",
    "corr_multiple = 1 - 1/np.diag(sigma_inv)/np.diag(sigma)\n",
    "print(\"Índice de correlación múltiple para cada variable explicativa:\")\n",
    "print(corr_multiple)\n",
    "\n",
    "### ciclo para eliminar variables con alta correlacion multiple\n",
    "X_reg_reduced = X_reg.copy()\n",
    "max_corr_multiple = max(corr_multiple)\n",
    "while max_corr_multiple > 0.8:\n",
    "    to_drop = np.argmax(corr_multiple)\n",
    "    print(f\"Eliminando variable {X_reg_reduced.columns[to_drop]} con índice de correlación múltiple {max_corr_multiple}\")\n",
    "    X_reg_reduced = X_reg_reduced.drop(columns = [X_reg_reduced.columns[to_drop]])\n",
    "    \n",
    "    sigma = X_reg_reduced.cov()\n",
    "    sigma_inv = np.linalg.inv(sigma)\n",
    "    corr_multiple = 1 - 1/np.diag(sigma_inv)/np.diag(sigma)\n",
    "    max_corr_multiple = max(corr_multiple)\n",
    "    print(\"Nuevo índice de correlación múltiple máximo tras eliminación:\", max_corr_multiple)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MCDA-aprendizaje-automatico",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
